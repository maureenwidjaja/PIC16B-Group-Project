{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6SXWqgrdAI-"
   },
   "source": [
    "# Scrape Books Using Open Library API using BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "3cfBooOJcRHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "1TLeu2GJc-RB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_books_by_subject(subject, limit=100, details=True, ebooks=False, published_in=None, offset=0):\n",
    "    '''\n",
    "    Args:\n",
    "    details: if True, includes related subjects, prolific authors, and publishers.\n",
    "    ebooks: if True,  filters for books with e-books.\n",
    "    published_in: filters by publication year.\n",
    "                  For example:\n",
    "                  http://openlibrary.org/subjects/love.json?published_in=1500-1600\n",
    "    limit: num of works to include in the response, controls pagination.\n",
    "    offset: starting offset in the total works, controls pagination.\n",
    "    '''\n",
    "    # Creates the API endpoint URL using the subject provided.\n",
    "    base_url = (f'https://openlibrary.org/subjects/{subject}.json?limit=1')\n",
    "\n",
    "\n",
    "    # Sends an HTTP GET request to Open Library's API with the query parameters\n",
    "    # stored in params.\n",
    "    # The response is stored in response, which contains JSON data.\n",
    "    response = requests.get(base_url)#, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching books for {subject}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    books = data.get(\"works\", [])\n",
    "\n",
    "    if not books:\n",
    "        print(f\"No books found for {subject}\")\n",
    "        return []\n",
    "\n",
    "    book_list = []\n",
    "    for book in books:\n",
    "        title = book.get(\"title\", \"Unknown Title\")\n",
    "        author = book[\"authors\"][0][\"name\"] if book.get(\"authors\") else \"Unknown Author\"\n",
    "        published_year = book.get(\"first_publish_year\", \"Unknown Year\")\n",
    "\n",
    "        # Other details we may need for\n",
    "        subjects = \", \".join(book.get(\"subject\", [\"No subjects available\"]))\n",
    "        description = book.get(\"description\", \"No description available\")\n",
    "        ebook_available = book.get(\"ebook_count_i\", 0) > 0\n",
    "        publishers = \", \".join(book.get(\"publishers\", [\"Unknown Publisher\"]))\n",
    "\n",
    "        book_list.append({\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"published_year\": published_year,\n",
    "            \"subjects\": subjects,\n",
    "            \"description\": description,\n",
    "            \"ebook_available\": ebook_available,\n",
    "            \"publishers\": publishers\n",
    "        })\n",
    "\n",
    "    return book_list  # Return the list of books\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZVUatKlOoMH",
    "tags": []
   },
   "source": [
    "# Combine into one genre\n",
    "\n",
    "For instance, \"sci-fi\" subject and \"science-fiction\" subject returns different results. So, our next objective is to combine all of them into one genre \"Science Fiction\". The same goes for other genres like \"Romance\" or \"Fantasy\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZei4rWfPJGk",
    "tags": []
   },
   "source": [
    "### Function to combine sub-genres into a big genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "UfwwK6xYQvEJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_genre(subject):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    subject: book subject\n",
    "\n",
    "    This function collects book lists under sub-genres and combines them into\n",
    "    one main genre.\n",
    "\n",
    "    Returns:\n",
    "    List of all books under a specific genre.\n",
    "    \"\"\"\n",
    "\n",
    "    if subject is None:\n",
    "        raise ValueError(\"Please pass a subject name.\")\n",
    "\n",
    "    # Dictionary of genres and their corresponding lists with adjusted formatting\n",
    "    genre_dict = {\n",
    "        \"romance\": [\n",
    "            \"fiction_romance_general\", \"fiction_romance_historical_general\",\n",
    "            \"romance\", \"man_woman_relationships\", \"fiction_romance_suspense\",\n",
    "            \"fiction_romance_contemporary\",\n",
    "            \"fiction_romance_erotica\", \"fiction_romance_erotic\",\n",
    "            \"marriage_fiction\", \"fiction_erotica_general\", \"romance\",\n",
    "            \"fiction_christian_romance_general\", \"fiction_romance_historical\"\n",
    "        ],\n",
    "        \"fantasy\": [\n",
    "            \"fiction\", \"fantasy_fiction\", \"magic\", \"fiction_fantasy_general\",\n",
    "            \"adventure_and_adventurers_fiction\",\n",
    "            \"adventure_and_adventurers\", \"good_and_evil\", \"fairies\", \"dragons\",\n",
    "            \"cartoons_and_comics\", \"witchcraft\", \"history\", \"wizards\", \"fairies_fiction\"\n",
    "        ],\n",
    "        \"historical_fiction\": [\n",
    "            \"fiction\", \"historical_fiction\", \"history\", \"fiction_historical_general\",\n",
    "            \"fiction_romance_historical_general\", \"fiction_historical\", \"fiction_general\",\n",
    "            \"fiction_romance_historical\", \"world_war_1939_1945\", \"great_britain_fiction\"\n",
    "        ],\n",
    "        \"horror\": [\n",
    "            \"fiction\", \"horror\", \"horror_stories\", \"horror_tales\", \"american_horror_tales\",\n",
    "            \"horror_fiction\", \"detective_and_mystery_stories\", \"crime\", \"catalepsy\", \"murder\",\n",
    "            \"burial_vaults\"\n",
    "        ],\n",
    "        \"humor\": [\n",
    "            \"anecdotes\", \"humor_general\", \"american_wit_and_humor\",\n",
    "            \"wit_and_humor\", \"humour\", \"humor\", \"funny\"\n",
    "        ],\n",
    "        \"literature\": [\n",
    "            \"philosophy\", \"in_literature\", \"theory\", \"criticism\", \"criticism_and_interpretation\",\n",
    "            \"english_literature\", \"modern_literature\", \"american_literature\",\n",
    "            \"literature\", \"litterature\"\n",
    "        ],\n",
    "        \"mystery_thriller\": [\n",
    "            \"detective_and_mystery_stories\", \"mystery_fiction\", \"murder\", \"mystery\",\n",
    "            \"thriller\", \"detective\", \"fiction_thrillers_general\",\n",
    "            \"suspense\", \"fiction_thrillers_suspense\", \"fiction_suspense\",\n",
    "            \"mystery\", \"thriller\", \"murder\",\n",
    "            \"fiction_thrillers_espionage\", \"police\",\n",
    "            \"suspense_fiction\", \"fiction_general\", \"detective_and_mystery_stories\",\n",
    "            \"crimes_against\", \"fiction_psychological\", \"investigation\"\n",
    "        ],\n",
    "        \"science_fiction\": [\n",
    "            \"science_fiction\", \"fiction_science_fiction_general\", \"american_science_fiction\",\n",
    "            \"extraterrestrial_beings\", \"life_on_other_planets\", \"extraterrestrial_beings_fiction\",\n",
    "            \"time_travel\", \"sci_fi\", \"sci-fi\", \"science-fiction\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if subject not in genre_dict:\n",
    "        raise ValueError(\"Invalid genre. Please choose from the predefined genres: \\\n",
    "        romance, fantasy, historical_fiction, horror, humor, literature, \\\n",
    "        mystery_thriller, science_fiction.\")\n",
    "\n",
    "    books_under_genre = []\n",
    "    seen_books = set()  # To store unique books\n",
    "    i = 1\n",
    "    print(f\"\\nBooks under the genre '{subject}':\\n\")\n",
    "\n",
    "    for sub_genre in genre_dict[subject]:\n",
    "        books = get_books_by_subject(sub_genre)  # Get books for the sub-genre\n",
    "\n",
    "        if books:\n",
    "            for book in books:\n",
    "                # Extract the book title and author for uniqueness check)\n",
    "                title_author = book['author']\n",
    "                if title_author is None:\n",
    "                  print(\"no author\")\n",
    "\n",
    "                # Ensure no dupicates\n",
    "                if title_author not in seen_books:\n",
    "                    print(f\"{i}. {book['title']} by {book['author']}\")\n",
    "                    books_under_genre.append(book)\n",
    "                    seen_books.add(title_author)\n",
    "                    i += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"No books found for sub-genre '{sub_genre}'\")\n",
    "\n",
    "    return books_under_genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ub8gV81QUu-"
   },
   "source": [
    "# Scrape User Ratings & Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "jInyi4pge6cG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "link = \"https://openlibrary.org/subjects\"\n",
    "data = requests.get(link).text\n",
    "response = requests.get(link)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "hWIPCexT1Z9P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def link2soup(link):\n",
    "    data = requests.get(link).text\n",
    "    return BeautifulSoup(data, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "KhOrVgb2XMfw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_to_books(link):\n",
    "    '''\n",
    "    Assume we start in \"https://openlibrary.org/subjects\" (base link) page,\n",
    "    crawls to each chosen subject link and add it to the base link.\n",
    "    '''\n",
    "    # assume start in \"https://openlibrary.org/subjects\" page\n",
    "    chosen_subjects = {\n",
    "        \"romance\", \"fantasy\", \"historical_fiction\", \"horror\", \"humor\",\n",
    "        \"literature\", \"mystery_and_detective_stories\", \"science_fiction\"\n",
    "    }\n",
    "\n",
    "    soup = link2soup(link)\n",
    "\n",
    "    # Extract all subjects links\n",
    "    all_links = [a['href'] for a in soup.select(\"div#subjectsPage ul li a\") if 'href' in a.attrs]\n",
    "    # print(all_links[:5])\n",
    "\n",
    "    # Filter only the chosen subjects\n",
    "    filtered_links = [\n",
    "        link for link in all_links\n",
    "        # ensure only parsing on wanted subjects\n",
    "        if any(sub in link for sub in chosen_subjects) and \"juvenile_literature\" not in link]\n",
    "    # print(filtered_links)\n",
    "\n",
    "    # assume we wish to click the 'fantasy' (specific subject) page\n",
    "    # crawling through links\n",
    "    specific_subject_urls = []\n",
    "    for i in range(len(filtered_links)):\n",
    "        next_button = filtered_links[i].split('/')[2]\n",
    "        specific_subject_url = link + \"/\" + next_button\n",
    "        specific_subject_urls.append(specific_subject_url)\n",
    "\n",
    "    return(specific_subject_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "yOls7kkbwKoa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assumes we want to go to the total \"works\" under each subject\n",
    "def total_works_page(specific_subject_urls):\n",
    "    '''assumes we want to go to the total \"works\" under each subject\n",
    "\n",
    "    Returns: all urls to 'total works' under each subject\n",
    "    '''\n",
    "    base_link = \"https://openlibrary.org\"\n",
    "    total_works_list = []\n",
    "\n",
    "    for url in specific_subject_urls:\n",
    "        soup = link2soup(url)\n",
    "\n",
    "        # Extract total works link\n",
    "        total_works_link = [a['href'].replace(\" \", \"%20\") for a in soup.select(\"a[title='See all works']\") if 'href' in a.attrs]\n",
    "\n",
    "        # Print extracted links (if any)\n",
    "        print(f\"Total works links for {url}: {total_works_link}\")\n",
    "\n",
    "        # Append full URLs to list:\n",
    "        for link in total_works_link:\n",
    "            total_works_list.append(base_link + link)\n",
    "\n",
    "    return total_works_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "upqReyWWer9k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_book_hrefs(url):\n",
    "    '''\n",
    "    Args: \n",
    "        url: takes a URL or list of URLs that it loops through\n",
    "    Returns: \n",
    "        endof_links: returns the hrefs of all the links on the page\n",
    "    \n",
    "    Notes:\n",
    "        if you only pass the first page of results for each function, \n",
    "        it will only scrape books from page 1\n",
    "    '''\n",
    "    soup = link2soup(url)\n",
    "    endof_links = [a['href'] for a in soup.select(\"div#searchResults li.searchResultItem.sri--w-main a.results\")\n",
    "                    if 'href' in a.attrs]\n",
    "    return endof_links\n",
    "    print(soup.prettify())\n",
    "\n",
    "# tried this function, only got the first page of the fantasy, therefore we need an every_page function\n",
    "# get_book_hrefs('https://openlibrary.org/search?subject=Fantasy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "sgDul2RJlZ-j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracts the link to the next pages of \"total works\" under a specific subject\n",
    "def every_page(subject_total_work_url):\n",
    "    '''\n",
    "    Assume we are in the first page of the 'total works' under each subject.\n",
    "    Parse pages from the first page to the last under each subject.\n",
    "\n",
    "    Return: URL of pages from 1 - n of 'total works' under each subject\n",
    "    '''\n",
    "    base_url = 'https://openlibrary.org'\n",
    "    page_links = []\n",
    "\n",
    "    for subject_url in subject_total_work_url:\n",
    "        soup = link2soup(subject_url)\n",
    "\n",
    "        # Find the last page number\n",
    "        last_page = soup.select_one('a.ChoosePage[data-ol-link-track=\"Pager|LastPage\"]')\n",
    "        if last_page:\n",
    "            last_page_number = int(last_page.get_text())\n",
    "            # last_page_number = 3 for now for debugging\n",
    "        else:\n",
    "            last_page_number = 1  # If there's only one page\n",
    "\n",
    "        # Generate all page links from 1 to last_page_number\n",
    "        for page in range(1, last_page_number + 1):\n",
    "            page_links.append(f'{subject_url}&page={page}')\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "hFhEn7HjCkxL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def book_links(urls):\n",
    "    # EDITED: book links really accepts a broad list of \"search\" style URLs\n",
    "    # use ThreadPoolExecutor to run code concurrently: this means that multiple\n",
    "    #   requests may go out and be pending at the same; max_workers is set to\n",
    "    #   100, but there might be a better value to pick!\n",
    "    # add tqdm as a progress bar\n",
    "    # there is still a concern regarding rate limiting: subsequent requests may\n",
    "    #   get slower and slower or just return with an error (HTTP 422)\n",
    "    '''\n",
    "    Assumes we are in the \"total works\" section of each subject and wish to parse over every book.\n",
    "    This also includes parsing the next pages continuously until the end.\n",
    "\n",
    "    Return: URL of every book under \"total works\" section of each subject\n",
    "    '''\n",
    "\n",
    "    base_url = \"https://openlibrary.org\"\n",
    "    book_links = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:\n",
    "        future_to_url = {executor.submit(get_book_hrefs, url): url for url in urls}\n",
    "        print(\"SUBMITTED\")\n",
    "        with tqdm(total=len(urls)) as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    book_links.extend(map(lambda href: base_url + href, data))\n",
    "                except Exception as exc:\n",
    "                    print('%r generated an exception: %s' % (url, exc))\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return book_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "IgWTOgREBc0C"
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def every_book(all_pages, subject_total_work_url):\n",
    "    \"\"\"\n",
    "    Assume we are in the first page of every 'total works' under a subject\n",
    "    and wish to parse through every book in every page under the 'total works'.\n",
    "\n",
    "    Returns: A list of URLs for every book from pages 1 to n under 'total works' of each subject.\n",
    "    \"\"\"\n",
    "    all_book_links = []\n",
    "\n",
    "    # with tqdm(total=len(all_pages)) as pbar:\n",
    "    #     with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    #         future_to_page = {executor.submit(book_links, [page]): page for page in all_pages}\n",
    "    #         for future in concurrent.futures.as_completed(future_to_page):\n",
    "    #             page = future_to_page[future]\n",
    "    #             try:\n",
    "    #                 data = future.result()\n",
    "    #                 all_book_links.extend(data)\n",
    "    #             except Exception as exc:\n",
    "    #                 print('%r generated an exception: %s' % (page, exc))\n",
    "    #             finally:\n",
    "    #                 pbar.update(1)\n",
    "\n",
    "    return all_book_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "Q7UnUNkyT-2f",
    "outputId": "29cdc20d-1803-484e-fc65-a7352b8bae66"
   },
   "outputs": [],
   "source": [
    "def extract_book_reviews(file):\n",
    "    '''\n",
    "    Args:\n",
    "        Accepts the 'all_book_links.csv', which has a column of all the book links of Open Library. \n",
    "    Returns:\n",
    "        Returns a csv file with all reviews in their columns.\n",
    "    Notes:\n",
    "        Extracts book reviews by categories (columns), paired with a percentage value. Returns a dataframe of books and\n",
    "    '''\n",
    "    subject_link = f'https://openlibrary.org/search?subject={subject}'\n",
    "    response = requests.get(subject_link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # extract book titles\n",
    "    book_titles = every_book(subject) # make sure that the every_book function works\n",
    "\n",
    "    # extract community reviews\n",
    "    community_reviews = [x.get_text() for x in soup.select(\"span.reviews__value\")]\n",
    "    community_data = [re.search(r'\\s*(\\w[\\w\\s]*)',x).group(1).strip() for x in community_reviews]\n",
    "\n",
    "    # extract percentages\n",
    "    review_percentage = [x.get_text() for x in soup.select(\"span.percentage\")]\n",
    "    percentage_data = [re.search(r'\\s*(\\w[\\w\\s]*)',x).group(1).strip() for x in review_percentage]\n",
    "\n",
    "    # ensure that community reviews and percentages (lists) are the same length\n",
    "    if len(community_data) != len(percentage_data):\n",
    "        print(\"The reviews and percentages do not match.\")\n",
    "\n",
    "    # find number of reviews\n",
    "    number_of_reviews = [x.get_text() for x in soup.select(\"h2.observation-title\")]\n",
    "    number_of_reviews = [re.search(r'\\((\\d+)\\)',x).group(1).strip() for x in number_of_reviews]\n",
    "\n",
    "    # create an empty dict\n",
    "    book_dict = {}\n",
    "\n",
    "    # convert the community reviews and percentages into a dictionary\n",
    "    for i in range(len(book_titles)): #loop through books\n",
    "        review_dict = {'Title' : book_titles[i]}\n",
    "\n",
    "        for j in range(len(community_data)):\n",
    "            categories = community_data[j]\n",
    "            percentage_value = review_percentage[j]\n",
    "\n",
    "            review_dict[categories] = percentage_value\n",
    "\n",
    "        book_dict.append(review_dict)\n",
    "\n",
    "    # convert the dictionary into dataframe\n",
    "    df = pd.Dataframe(book_dict)\n",
    "\n",
    "    # return the dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR4rZxUZpVip"
   },
   "source": [
    "# What to do next:\n",
    "1. Build ML model\n",
    "  - training data: csv file containing books in a specific genre?\n",
    "  - testing data: our prediction now?\n",
    "\n",
    "2. Approaches to consider:\n",
    "  - Collaborative Filtering (based on user ratings, user reviews e.g. Goodreads)\n",
    "  - Content-Based Filtering (based on genre, content description, etc.)\n",
    "  - Combination of both Filtering Methods\n",
    "\n",
    "3. Define Training Data\n",
    "  - What should the csv file include?\n",
    "    1. Book Information: Book ID, Title, Author, Genres, Description\n",
    "    2. User Ratings: User ID, Book ID, Rating, User Reviews\n",
    "\n",
    "4. Machine Learning Models to consider:\n",
    "  - Content-Based Filtering: Book descriptions and genres\n",
    "      - TF-IDF (Term Frequency-Inverse Document Frequency): evaluates the importance of a word in a document : https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\n",
    "      - Sci-Kit Learn: classifiers, feature-extraction\n",
    "  - Collaborative Filtering: User ratings and reviews\n",
    "      - Single Value Decomposition (SVD): can decompose a matrix into 3 matrices, good for ratings: https://www.geeksforgeeks.org/singular-value-decomposition-svd/\n",
    "  - From surprise: https://surpriselib.com/\n",
    "\n",
    "\n",
    "5. Hybrid model\n",
    "  - Step 1: Get the top books for the user through collaborative filtering\n",
    "  - Step 2: Find the most similar books through content based filtering\n",
    "  - Step 3: Return the list of recommended books\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
